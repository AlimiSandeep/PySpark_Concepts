{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('PySparkLearning').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000), \\\n",
    "        (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n",
    "        (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n",
    "        (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n",
    "        (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n",
    "        (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n",
    "  ]\n",
    "\n",
    "empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \\\n",
    "       \"emp_dept_id\",\"gender\",\"salary\"]\n",
    "\n",
    "\n",
    "dept = [(\"Finance\",10), \\\n",
    "        (\"Marketing\",20), \\\n",
    "        (\"Sales\",30), \\\n",
    "        (\"IT\",40) \\\n",
    "  ]\n",
    "\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- superior_emp_id: long (nullable = true)\n",
      " |-- year_joined: string (nullable = true)\n",
      " |-- emp_dept_id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "root\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- dept_id: long (nullable = true)\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF = spark.createDataFrame(data=emp, schema = empColumns)\n",
    "empDF.printSchema()\n",
    "empDF.show(truncate=False)\n",
    "\n",
    "print('<'*50, '>'*50)\n",
    "\n",
    "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
    "deptDF.printSchema()\n",
    "deptDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark Inner Join DataFrame\n",
    "\n",
    "Inner join is the default join in PySpark and it’s mostly used. This joins two datasets on key columns, where keys don’t match the rows get dropped from both datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF, empDF.emp_dept_id ==  deptDF.dept_id,\"inner\") \\\n",
    "     .orderBy('emp_id')\\\n",
    "     .show(truncate=False)\n",
    "\n",
    "# When we apply Inner join on our datasets, It drops “emp_dept_id” 50 from “emp” and “dept_id” 30 from “dept” datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PySpark Left Outer Join\n",
    "\n",
    "Left a.k.a Leftouter join returns all rows from the left dataset regardless of match found on the right dataset. When join expression( join condition ) doesn’t match, it assigns `null` for that record and drops records from right dataset where match is not found.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|     1|   Smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n",
      "|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n",
      "|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n",
      "|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n",
      "|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n",
      "|     6|   Brown|              2|       2010|         50|      |    -1|     null|   null|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|     1|   Smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n",
      "|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n",
      "|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n",
      "|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n",
      "|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n",
      "|     6|   Brown|              2|       2010|         50|      |    -1|     null|   null|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF, empDF.emp_dept_id ==  deptDF.dept_id, \"left\")\\\n",
    "     .orderBy('emp_id')\\\n",
    "     .show()\n",
    "\n",
    "print('<'*50, '>'*50)\n",
    "\n",
    "empDF.join(deptDF, empDF.emp_dept_id ==  deptDF.dept_id, \"leftouter\")\\\n",
    "     .orderBy('emp_id')\\\n",
    "     .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our dataset, “emp_dept_id” 50 doesn’t have a record on “dept” dataset hence, this record contains null on “dept” columns (dept_name & dept_id). \n",
    "And “dept_id” 30 from “dept” dataset dropped from the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Right Outer Join\n",
    "\n",
    "Right a.k.a Rightouter join is opposite of left join, here it returns all rows from the right dataset regardless of math found on the left dataset. When join expression( join condition ) doesn’t match, it assigns null for that record and drops records from left dataset where match is not found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|  null|    null|           null|       null|       null|  null|  null|    Sales|     30|\n",
      "|     1|   Smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n",
      "|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n",
      "|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n",
      "|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n",
      "|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|  null|    null|           null|       null|       null|  null|  null|    Sales|     30|\n",
      "|     1|   Smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n",
      "|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n",
      "|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n",
      "|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n",
      "|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF, empDF.emp_dept_id ==  deptDF.dept_id, \"right\")\\\n",
    "     .orderBy('emp_id')\\\n",
    "     .show()\n",
    "\n",
    "print('<'*50, '>'*50)\n",
    "\n",
    "empDF.join(deptDF, empDF.emp_dept_id ==  deptDF.dept_id, \"rightouter\")\\\n",
    "     .orderBy('emp_id')\\\n",
    "     .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our example, the right dataset “dept_id” 30 doesn’t have it on the left dataset “emp” hence, this record contains null on “emp” columns. and “emp_dept_id” 50 dropped as a match not found on left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark Full Outer Join\n",
    "\n",
    "Outer a.k.a full, fullouter join returns all rows from both datasets, where join expression doesn’t match it returns null on respective record columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|  null|    null|           null|       null|       null|  null|  null|    Sales|     30|\n",
      "|     1|   Smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n",
      "|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n",
      "|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n",
      "|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n",
      "|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n",
      "|     6|   Brown|              2|       2010|         50|      |    -1|     null|   null|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|  null|    null|           null|       null|       null|  null|  null|    Sales|     30|\n",
      "|     1|   Smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n",
      "|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n",
      "|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n",
      "|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n",
      "|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n",
      "|     6|   Brown|              2|       2010|         50|      |    -1|     null|   null|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|  null|    null|           null|       null|       null|  null|  null|    Sales|     30|\n",
      "|     1|   Smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n",
      "|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n",
      "|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n",
      "|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n",
      "|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n",
      "|     6|   Brown|              2|       2010|         50|      |    -1|     null|   null|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF, empDF.emp_dept_id ==  deptDF.dept_id, \"outer\")\\\n",
    "     .orderBy('emp_id')\\\n",
    "     .show()\n",
    "\n",
    "print('<'*50, '>'*50)\n",
    "\n",
    "empDF.join(deptDF, empDF.emp_dept_id ==  deptDF.dept_id, \"full\")\\\n",
    "     .orderBy('emp_id')\\\n",
    "     .show()\n",
    "print('<'*50, '>'*50)\n",
    "\n",
    "empDF.join(deptDF, empDF.emp_dept_id ==  deptDF.dept_id, \"fullouter\")\\\n",
    "     .orderBy('emp_id')\\\n",
    "     .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our “emp” dataset’s “emp_dept_id” with value 50 doesn’t have a record on “dept” hence dept columns have null and “dept_id” 30 doesn’t have a record in “emp” hence you see null’s on emp columns.\n",
    "\n",
    "In other words combines the result of both left join and right join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left Semi Join\n",
    "\n",
    "`leftsemi` join is similar to `inner join` difference being - leftsemi join returns all columns from the left dataset and ignores all columns from the right dataset. In other words, this join returns columns from the only left dataset for the records match in the right dataset on join expression, records not matched on join expression are ignored from both left and right datasets.\n",
    "\n",
    "The same result can be achieved using `select` on the result of the inner join however, using this join would be efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftsemi\") \\\n",
    "     .orderBy('emp_id')\\\n",
    "     .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left Anti Join\n",
    "\n",
    "`leftanti` join does the exact opposite of the `leftsemi`, leftanti join returns only columns from the left dataset for non-matched records.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---------------+-----------+-----------+------+------+\n",
      "|emp_id|name |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
      "+------+-----+---------------+-----------+-----------+------+------+\n",
      "|6     |Brown|2              |2010       |50         |      |-1    |\n",
      "+------+-----+---------------+-----------+-----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftanti\") \\\n",
    "     .orderBy('emp_id')\\\n",
    "     .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark Self Join\n",
    "\n",
    "Joins are not complete without a self join, Though there is no self-join type available, we can use any of the above-explained join types to join DataFrame to itself. below example use inner self join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------------+\n",
      "|emp_id|name    |superior_emp_id|superior_emp_name|\n",
      "+------+--------+---------------+-----------------+\n",
      "|2     |Rose    |1              |Smith            |\n",
      "|3     |Williams|1              |Smith            |\n",
      "|4     |Jones   |2              |Rose             |\n",
      "|5     |Brown   |2              |Rose             |\n",
      "|6     |Brown   |2              |Rose             |\n",
      "+------+--------+---------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col \n",
    "\n",
    "empDF.alias(\"emp1\").join(empDF.alias(\"emp2\"), \\\n",
    "      col(\"emp1.superior_emp_id\") == col(\"emp2.emp_id\"),\"inner\") \\\n",
    "     .select(col(\"emp1.emp_id\"),col(\"emp1.name\"), \\\n",
    "             col(\"emp2.emp_id\").alias(\"superior_emp_id\"), \\\n",
    "             col(\"emp2.name\").alias(\"superior_emp_name\")) \\\n",
    "     .orderBy('emp_id')\\\n",
    "     .show(truncate=False)\n",
    "\n",
    "# Here, we are joining emp dataset with itself to find out superior_emp_id and name for all employees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SQL Expression\n",
    "\n",
    "Since PySpark SQL support native SQL syntax, we can also write join operations after creating temporary tables on DataFrame’s and use these tables on `spark.sql().`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.createOrReplaceTempView(\"EMP\")\n",
    "deptDF.createOrReplaceTempView(\"DEPT\")\n",
    "\n",
    "joinDF1 = spark.sql(\"select * from EMP e, DEPT d where e.emp_dept_id == d.dept_id\") \\\n",
    "  .show(truncate=False)\n",
    "\n",
    "joinDF2 = spark.sql(\"select * from EMP e INNER JOIN DEPT d ON e.emp_dept_id == d.dept_id\") \\\n",
    "  .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark SQL Join on multiple DataFrame’s\n",
    "\n",
    "When you need to join more than two tables, you either use SQL expression after creating a temporary view on the DataFrame or use the result of join operation to join with another DataFrame like chaining them. for example\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "df1.join(df2,df1.id1 == df2.id2,\"inner\") \n",
    "   .join(df3,df1.id1 == df3.id3,\"inner\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
