{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('PySparkLearning').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Row Object\n",
    "Row class extends the tuple hence it takes variable number of arguments, Row() is used to create the row object. Once the row object created, we can retrieve the data from Row using index similar to tuple.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "James,40\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "row=Row(\"James\",40)\n",
    "print(row[0] +\",\"+str(row[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively you can also write with named arguments. Benefits with the named argument is you can access with field name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sandeep 24\n"
     ]
    }
   ],
   "source": [
    "row = Row(name='Sandeep', age = 24)\n",
    "\n",
    "print(row.name + \" \"+ str(row.age))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create custom class from Row\n",
    "\n",
    "We can also create a Row like class, for example “Person” and use it similar to Row object. This would be helpful when you wanted to create real time object and refer it’s properties. On below example, we have created a Person class and used similar to Row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sandeep -> 24\n"
     ]
    }
   ],
   "source": [
    "Person = Row(\"name\", \"age\")\n",
    "\n",
    "p1 = Person('Sandeep',24)\n",
    "print(p1.name+\" -> \"+str(p1.age))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Row class on PySpark RDD\n",
    "\n",
    "We can use Row class on PySpark RDD. When you use Row to create an RDD, after collecting the data you will get the result back in Row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(name='James,,Smith', lang=['Java', 'Scala', 'C++'], state='CA'), Row(name='Michael,Rose,', lang=['Spark', 'Java', 'C++'], state='NJ'), Row(name='Robert,,Williams', lang=['CSharp', 'VB'], state='NV')]\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "        Row(name=\"James,,Smith\",lang=[\"Java\",\"Scala\",\"C++\"],state=\"CA\"), \n",
    "        Row(name=\"Michael,Rose,\",lang=[\"Spark\",\"Java\",\"C++\"],state=\"NJ\"),\n",
    "        Row(name=\"Robert,,Williams\",lang=[\"CSharp\",\"VB\"],state=\"NV\")\n",
    "       ]\n",
    "\n",
    "rdd=spark.sparkContext.parallelize(data)\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "James,,Smith,['Java', 'Scala', 'C++']\n",
      "Michael,Rose,,['Spark', 'Java', 'C++']\n",
      "Robert,,Williams,['CSharp', 'VB']\n"
     ]
    }
   ],
   "source": [
    "for row in rdd.collect():\n",
    "    print(row.name + \",\" +str(row.lang))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can also do by creating a Row like class “Person”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(name='James,,Smith', lang=['Java', 'Scala', 'C++'], state='CA'), Row(name='Michael,Rose,', lang=['Spark', 'Java', 'C++'], state='NJ'), Row(name='Robert,,Williams', lang=['CSharp', 'VB'], state='NV')]\n"
     ]
    }
   ],
   "source": [
    "Person=Row(\"name\",\"lang\",\"state\")\n",
    "\n",
    "data = [\n",
    "        Person(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \n",
    "        Person(\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"),\n",
    "        Person(\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\")\n",
    "    ]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Row class on PySpark DataFrame\n",
    "\n",
    "Similarly, Row class also can be used with PySpark DataFrame, By default data in DataFrame represent as Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- lang: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n",
      "+----------------+------------------+-----+\n",
      "|            name|              lang|state|\n",
      "+----------------+------------------+-----+\n",
      "|    James,,Smith|[Java, Scala, C++]|   CA|\n",
      "|   Michael,Rose,|[Spark, Java, C++]|   NJ|\n",
      "|Robert,,Williams|      [CSharp, VB]|   NV|\n",
      "+----------------+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "        Row(name=\"James,,Smith\",lang=[\"Java\",\"Scala\",\"C++\"],state=\"CA\"), \n",
    "        Row(name=\"Michael,Rose,\",lang=[\"Spark\",\"Java\",\"C++\"],state=\"NJ\"),\n",
    "        Row(name=\"Robert,,Williams\",lang=[\"CSharp\",\"VB\"],state=\"NV\")\n",
    "       ]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also change the column names by using `toDF()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- prog_lang: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- current_state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"name\",\"prog_lang\",\"current_state\"]\n",
    "df=spark.createDataFrame(data).toDF(*columns)\n",
    "df.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
