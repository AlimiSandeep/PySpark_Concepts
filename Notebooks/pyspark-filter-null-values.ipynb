{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .master(\"local[1]\") \\\n",
    "                    .appName(\"PySparkLearning\") \\\n",
    "                    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+\n",
      "| name|state|gender|\n",
      "+-----+-----+------+\n",
      "|James| null|     M|\n",
      "| Anna|   NY|     F|\n",
      "|Julia| null|  null|\n",
      "+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "        (\"James\",None,\"M\"),\n",
    "        (\"Anna\",\"NY\",\"F\"),\n",
    "        (\"Julia\",None,None)\n",
    "  ]\n",
    "\n",
    "columns = [\"name\",\"state\",\"gender\"]\n",
    "\n",
    "df = spark.createDataFrame(data,columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Rows with NULL Values in DataFrame\n",
    "\n",
    "In PySpark, using `filter() or where()` functions of DataFrame we can filter rows with NULL values by checking `IS NULL or isNULL`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+\n",
      "| name|state|gender|\n",
      "+-----+-----+------+\n",
      "|James| null|     M|\n",
      "|Julia| null|  null|\n",
      "+-----+-----+------+\n",
      "\n",
      "+-----+-----+------+\n",
      "| name|state|gender|\n",
      "+-----+-----+------+\n",
      "|James| null|     M|\n",
      "|Julia| null|  null|\n",
      "+-----+-----+------+\n",
      "\n",
      "+-----+-----+------+\n",
      "| name|state|gender|\n",
      "+-----+-----+------+\n",
      "|James| null|     M|\n",
      "|Julia| null|  null|\n",
      "+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.filter(\"state is NULL\").show()\n",
    "df.filter(df.state.isNull()).show()\n",
    "df.filter(col(\"state\").isNull()).show()\n",
    "\n",
    "# Removes all rows with null values on `state` column and returns the new DataFrame. \n",
    "# All the above returns the same output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Rows with NULL on Multiple Columns\n",
    "\n",
    "Letâ€™s see how to filter rows with NULL values on multiple columns in DataFrame. In order to do so you can use either `AND or && operators`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+\n",
      "| name|state|gender|\n",
      "+-----+-----+------+\n",
      "|Julia| null|  null|\n",
      "+-----+-----+------+\n",
      "\n",
      "+-----+-----+------+\n",
      "| name|state|gender|\n",
      "+-----+-----+------+\n",
      "|Julia| null|  null|\n",
      "+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"state IS NULL AND gender IS NULL\").show()\n",
    "df.filter(df.state.isNull() & df.gender.isNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Rows with IS NOT NULL or isNotNull\n",
    "\n",
    "`IS NOT NULL or isNotNull` is used to filter rows that are `NOT NULL` in PySpark DataFrame columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+\n",
      "|name|state|gender|\n",
      "+----+-----+------+\n",
      "|Anna|   NY|     F|\n",
      "+----+-----+------+\n",
      "\n",
      "+----+-----+------+\n",
      "|name|state|gender|\n",
      "+----+-----+------+\n",
      "|Anna|   NY|     F|\n",
      "+----+-----+------+\n",
      "\n",
      "+----+-----+------+\n",
      "|name|state|gender|\n",
      "+----+-----+------+\n",
      "|Anna|   NY|     F|\n",
      "+----+-----+------+\n",
      "\n",
      "+----+-----+------+\n",
      "|name|state|gender|\n",
      "+----+-----+------+\n",
      "|Anna|   NY|     F|\n",
      "+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"state IS NOT NULL\").show()\n",
    "df.filter(\"NOT state IS NULL\").show()\n",
    "df.filter(df.state.isNotNull()).show()\n",
    "df.filter(col(\"state\").isNotNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can also write the same using `df.na.drop()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+\n",
      "|name|state|gender|\n",
      "+----+-----+------+\n",
      "|Anna|   NY|     F|\n",
      "+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(subset=[\"state\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark SQL Filter Rows with NULL Values\n",
    "\n",
    "If you are familiar with PySpark SQL, you can check `IS NULL and IS NOT NULL` to filter the rows from DataFrame.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+\n",
      "| name|state|gender|\n",
      "+-----+-----+------+\n",
      "|James| null|     M|\n",
      "|Julia| null|  null|\n",
      "+-----+-----+------+\n",
      "\n",
      "+-----+-----+------+\n",
      "| name|state|gender|\n",
      "+-----+-----+------+\n",
      "|Julia| null|  null|\n",
      "+-----+-----+------+\n",
      "\n",
      "+----+-----+------+\n",
      "|name|state|gender|\n",
      "+----+-----+------+\n",
      "|Anna|   NY|     F|\n",
      "+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"DATA\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM DATA where STATE IS NULL\").show()\n",
    "spark.sql(\"SELECT * FROM DATA where STATE IS NULL AND GENDER IS NULL\").show()\n",
    "spark.sql(\"SELECT * FROM DATA where STATE IS NOT NULL\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
