{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('PySparkLearning').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"Z\", 1),(\"A\", 20),(\"B\", 30),(\"C\", 40),(\"B\", 30),(\"B\", 60)]\n",
    "inputRDD = spark.sparkContext.parallelize(data)\n",
    "  \n",
    "listRdd = spark.sparkContext.parallelize([1,2,3,4,5,3,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### aggregate() \n",
    "aggregates the elements of each partition, and then the results for all the partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate Function definition in PySpark\n",
    "\n",
    "**aggregate(zeroValue, seqOp, combOp)**\n",
    "\n",
    "`zeroValue` is the initial value for the accumulated result of each partition for the seqOp operator, and also the initial value for the combine results from different partitions for the combOp operator - this will typically be the neutral element (e.g. Null for list concatenation or 0 for summation). In short, it will be used in the both operations\n",
    "\n",
    "`seqOp`: seqOp is an operator used to accumulate results within a partition. This operation will be applied to  elements in all the partitions. (ie The operation you want to apply to RDD records. Runs once for every record in a partition.)\n",
    "\n",
    "`combOp`: combOp is an operator used to combine results from different partitions. (ie Defines how the resulted objects (one for every partition), should combined)\n",
    "\n",
    "Overall, seqOp will aggregate the elements from all the partitions, and combOp will merge all the result of seqOp in all the partitions. Both of the operation share the same initial values which is called zeroValue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "seqOp = (lambda x, y: x + y)\n",
    "combOp = (lambda x, y: x + y)\n",
    "agg=listRdd.aggregate(0, seqOp, combOp)\n",
    "print(agg) \n",
    "# output 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 7)\n"
     ]
    }
   ],
   "source": [
    "seqOp2 = (lambda x, y: (x[0] + y, x[1] + 1))\n",
    "combOp2 = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "agg2=listRdd.aggregate((0, 0), seqOp2, combOp2)\n",
    "print(agg2) \n",
    "# output (20,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  One more example with explanation\n",
    "list_RDD = spark.sparkContext.parallelize([1,2,3,4], 2)\n",
    "seqOp = (lambda local_result, list_element: (local_result[0] + list_element, local_result[1] + 1) )\n",
    "combOp = (lambda some_local_result, another_local_result: (some_local_result[0] + another_local_result[0], some_local_result[1] + another_local_result[1]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 4)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_RDD.aggregate( (0, 0), seqOp, combOp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, I gave descriptive names to my variables, but let me explain it further:\n",
    "\n",
    "The first partition has the sublist [1, 2]. We will apply the `seqOp` to each element of that list and this will produce a local result, a pair of (sum, length), that will reflect the result locally, only in that first partition.\n",
    "\n",
    "So, let's start: local_result gets initialized to the zeroValue parameter we provided the aggregate() with, i.e. (0, 0) and list_element is the first element of the list, i.e. 1. As a result this is what happens:\n",
    "\n",
    "```\n",
    "0 + 1 = 1\n",
    "0 + 1 = 1\n",
    "```\n",
    "Now, the `local result` is (1, 1), that means, that so far, for the 1st partition, after processing only the first element, the sum is 1 and the length 1. Notice, that `local_result` gets updated from (0, 0), to (1, 1).\n",
    "```\n",
    "1 + 2 = 3\n",
    "1 + 1 = 2\n",
    "```\n",
    "and now the `local result` is (3, 2), which will be the final result from the 1st partition, since they are no other elements in the sublist of the 1st partition.\n",
    "\n",
    "Doing the same for 2nd partition, we get (7, 2).\n",
    "\n",
    "Now we apply the combOp to each local result, so that we can form, the final, global result, like this: (3,2) + (7,2) = (10, 4)\n",
    "\n",
    "Example described in 'figure':\n",
    "\n",
    "\n",
    "<img src=\"../Reference%20Images/rdd-aggregate-functionality.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**treeAggregate()** \n",
    "\n",
    "– Aggregates the elements of this RDD in a multi-level tree pattern. The output of this function will be similar to the aggregate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "seqOp = (lambda x, y: x + y)\n",
    "combOp = (lambda x, y: x + y)\n",
    "agg2=listRdd.treeAggregate(0,seqOp, combOp)\n",
    "print(agg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**fold()**\n",
    "\n",
    "– Aggregate the elements of each partition, and then the results for all the partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "foldRes=listRdd.fold(0, add)\n",
    "print(foldRes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**reduce()**\n",
    "\n",
    "– Reduces the elements of the dataset using the specified binary operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "redRes=listRdd.reduce(add)\n",
    "print(redRes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**treeReduce()**\n",
    "\n",
    "– Reduces the elements of this RDD in a multi-level tree pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "# treeReduce. This is similar to reduce\n",
    "add = lambda x, y: x + y\n",
    "redRes=listRdd.treeReduce(add)\n",
    "print(redRes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**collect()**\n",
    "\n",
    "-Return the complete dataset as an Array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 3, 2]\n"
     ]
    }
   ],
   "source": [
    "data = listRdd.collect()\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`count()` – Return the count of elements in the dataset.\n",
    "\n",
    "`countApprox()` – Return approximate count of elements in the dataset, this method returns incomplete when execution time meets timeout.\n",
    "\n",
    "`countApproxDistinct()` – Return an approximate number of distinct elements in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count : 7\n",
      "countApprox : 7\n",
      "CountApproxDistinct : 5\n"
     ]
    }
   ],
   "source": [
    "# count, countApprox, countApproxDistinct\n",
    "print(\"Count : \"+str(listRdd.count()))\n",
    "print(\"countApprox : \"+str(listRdd.countApprox(1200)))\n",
    "print(\"CountApproxDistinct : \"+str(listRdd.countApproxDistinct()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`countByValue()` – Return Map[T,Long] key representing each unique value in dataset and value represents count each value present.\n",
    "\n",
    "`countByValueApprox()` – Same as countByValue() but returns approximate result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "countByValue :  defaultdict(<class 'int'>, {1: 1, 2: 2, 3: 2, 4: 1, 5: 1})\n"
     ]
    }
   ],
   "source": [
    "print(\"countByValue :  \"+str(listRdd.countByValue()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`first()` – Return the first element in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first :  1\n",
      "first :  ('Z', 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"first :  \"+str(listRdd.first()))\n",
    "print(\"first :  \"+str(inputRDD.first()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`top()` – Return top n elements from the dataset.\n",
    "\n",
    "Note: Use this method only when the resulting array is small, as all the data is loaded into the driver’s memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top : [5, 4]\n",
      "top : [('Z', 1), ('C', 40)]\n"
     ]
    }
   ],
   "source": [
    "print(\"top : \"+str(listRdd.top(2)))\n",
    "print(\"top : \"+str(inputRDD.top(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`min()` – Return the minimum value from the dataset.\n",
    "`max()` – Return the maximum value from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min :  1\n",
      "min :  ('A', 20)\n",
      "max :  5\n",
      "max :  ('Z', 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"min :  \"+str(listRdd.min()))\n",
    "print(\"min :  \"+str(inputRDD.min()))\n",
    "\n",
    "print(\"max :  \"+str(listRdd.max()))\n",
    "print(\"max :  \"+str(inputRDD.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`take()` – Return the first num elements of the dataset.\n",
    "\n",
    "`takeOrdered()` – Return the first num (smallest) elements from the dataset and this is the opposite of the take() action.\n",
    "Note: Use this method only when the resulting array is small, as all the data is loaded into the driver’s memory.\n",
    "\n",
    "`takeSample()` – Return the subset of the dataset in an Array.\n",
    "Note: Use this method only when the resulting array is small, as all the data is loaded into the driver’s memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "take : [1, 2]\n",
      "takeOrdered : [1, 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"take : \"+str(listRdd.take(2)))\n",
    "print(\"takeOrdered : \"+ str(listRdd.takeOrdered(2)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
